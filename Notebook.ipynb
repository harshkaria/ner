{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGwK6vImdv7r",
        "outputId": "f04b45b9-9411-4b57-f83d-7e92c17b1095"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: __MACOSX/._data         \n",
            "  inflating: data/.DS_Store          \n",
            "  inflating: __MACOSX/data/._.DS_Store  \n",
            "  inflating: data/test               \n",
            "  inflating: __MACOSX/data/._test    \n",
            "  inflating: data/train              \n",
            "  inflating: __MACOSX/data/._train   \n",
            "  inflating: data/dev                \n",
            "  inflating: __MACOSX/data/._dev     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GqB2MpoAT4Kj"
      },
      "outputs": [],
      "source": [
        "# Find all sentences from the training data\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_sentences_and_states(file_path, vocab = None):\n",
        "  sentences = []\n",
        "  states = []\n",
        "  states_set = set()\n",
        "  if vocab == None:\n",
        "    vocab = set()\n",
        "  with open(file_path, \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "    # singular sentence\n",
        "    sentence = []\n",
        "    # singular tag of format (prev tag, curr tag)\n",
        "    state_array = []\n",
        "    for line in lines:\n",
        "      # New sentence\n",
        "      if len(line.strip()) == 0 and len(sentence) > 0:\n",
        "        sentences.append([_ for _ in sentence])\n",
        "        states.append([_ for _ in state_array])\n",
        "        # print(sentences, states)\n",
        "        sentence = []\n",
        "        state_array = []\n",
        "        continue\n",
        "      word_split = line.strip().split(\" \")\n",
        "      # print(word_split)\n",
        "      word = word_split[1] if len(word_split) >= 2 else None\n",
        "      curr_state = word_split[2] if len(word_split) >= 3 else None\n",
        "      if curr_state not in states_set:\n",
        "        states_set.add(curr_state)\n",
        "      if word not in vocab:\n",
        "        vocab.add(word)\n",
        "      # print(word, curr_state)\n",
        "      sentence.append(word)\n",
        "      state_array.append(curr_state)\n",
        "    sentences.append([_ for _ in sentence])\n",
        "    states.append([_ for _ in state_array])\n",
        "    return sentences, states, states_set, vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sentences_and_tokens(sentences, tokens, true_tags):\n",
        "  decoded_sents = []\n",
        "  decoded_tokens = []\n",
        "  true_tokens = []\n",
        "  for sentence, token_arr, true_token_arr in zip(sentences, tokens, true_tags):\n",
        "    sentence_decoded = [idx2word[idx] for idx in sentence]\n",
        "    token_decoded = [idx2state[idx] for idx in token_arr]\n",
        "    true_tags_decoded = [idx2state[idx] for idx in true_token_arr]\n",
        "    decoded_sents.extend([sentence_decoded])\n",
        "    decoded_tokens.extend([token_decoded])\n",
        "    true_tokens.extend([true_tags_decoded])\n",
        "  return decoded_sents, decoded_tokens, true_tokens"
      ],
      "metadata": {
        "id": "j_dbv6SEY9MQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detach(all_sentences, decoded_tags, gt):\n",
        "  all_sentences = [torch.squeeze(sentence, 0) for sentence in all_sentences]\n",
        "  all_sentences = [sentence.cpu().detach().numpy() for sentence in all_sentences]\n",
        "  out_states = [tokens.cpu().detach().numpy() for tokens in decoded_tags]\n",
        "  gt_states = [tokens.cpu().detach().numpy() for tokens in gt]\n",
        "  return all_sentences, out_states, gt_states"
      ],
      "metadata": {
        "id": "TFfSN59fbnK0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_inference_to_file(decoded_tags, true_tags, test_sentences, filename, eval = True):\n",
        "    with open(filename, \"w\") as file:\n",
        "        for sentence, decoded_sentence_tags, gt_tags in zip(test_sentences, decoded_tags, true_tags):\n",
        "            for i, (word, pred, gt) in enumerate(zip(sentence, decoded_sentence_tags, gt_tags), 1):\n",
        "                if eval:\n",
        "                  line = f'{i} {word} {gt} {pred}'\n",
        "                else:\n",
        "                  line = f'{i} {word} {pred}'\n",
        "                file.write(line + '\\n')\n",
        "            file.write('\\n')\n"
      ],
      "metadata": {
        "id": "9gBSFhsFfxNd"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, states, all_states, vocab = get_sentences_and_states(\"./data/train\")\n",
        "dev_sentences, dev_states, _, vocab = get_sentences_and_states(\"./data/dev\", vocab)\n",
        "test_sentences, test_states, _, vocab = get_sentences_and_states(\"./data/test\", vocab)"
      ],
      "metadata": {
        "id": "J2zayDc-yXqd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No tags provided for test data\n",
        "for sublist in test_states:\n",
        "    # Iterate through each element in the sublist\n",
        "    for i in range(len(sublist)):\n",
        "        # Set the element to '<PAD>'\n",
        "        sublist[i] = '<PAD>'"
      ],
      "metadata": {
        "id": "RDLi7EmEzO_g"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {word: idx for idx, word in enumerate(vocab, 1)}\n",
        "state2idx = {state: idx for idx, state in enumerate(all_states, 1)}\n",
        "idx2word = {idx: word for idx, word in enumerate(vocab, 1)}\n",
        "idx2state = {idx: state for idx, state in enumerate(all_states, 1)}"
      ],
      "metadata": {
        "id": "exy__ZwQatZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx.update({\"<PAD>\": 0})\n",
        "state2idx.update({\"<PAD>\": 0})\n",
        "idx2word.update({0: \"<PAD>\"})\n",
        "idx2state.update({0: \"<PAD>\"})"
      ],
      "metadata": {
        "id": "0qLU3Fsj6B6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_char_vocab(sentences):\n",
        "    \"\"\"\n",
        "    Creates a character-level vocabulary from a list of sentences.\n",
        "\n",
        "    Args:\n",
        "        sentences (list): A list of sentences.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two dictionaries:\n",
        "            char2idx (dict): A dictionary mapping characters to their integer indices.\n",
        "            idx2char (dict): A dictionary mapping integer indices to their characters.\n",
        "    \"\"\"\n",
        "    vocab = set()\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            for char in word:\n",
        "              vocab.add(char)\n",
        "    vocab = sorted(vocab)\n",
        "    char2idx = {char: i for i, char in enumerate(vocab)}\n",
        "    idx2char = {i: char for i, char in enumerate(vocab)}\n",
        "    return char2idx, idx2char"
      ],
      "metadata": {
        "id": "iJ-w7Ekyoebp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def save_dicts_to_json(word2idx, state2idx, idx2word, idx2state):\n",
        "    # Save word2idx as JSON\n",
        "    with open('word2idx.json', 'w') as f:\n",
        "        json.dump(word2idx, f)\n",
        "\n",
        "    # Save state2idx as JSON\n",
        "    with open('state2idx.json', 'w') as f:\n",
        "        json.dump(state2idx, f)\n",
        "\n",
        "    # Save idx2word as JSON\n",
        "    with open('idx2word.json', 'w') as f:\n",
        "        json.dump(idx2word, f)\n",
        "\n",
        "    # Save idx2state as JSON\n",
        "    with open('idx2state.json', 'w') as f:\n",
        "        json.dump(idx2state, f)"
      ],
      "metadata": {
        "id": "Ojg5mLxZFfv-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_char_dicts_to_json(char2idx, idx2char):\n",
        "  with open('char2idx.json', 'w') as f:\n",
        "    json.dump(char2idx, f)\n",
        "  with open('idx2char.json', 'w') as f:\n",
        "    json.dump(idx2char, f)"
      ],
      "metadata": {
        "id": "qm2u2uXdoo_Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def load_chars_from_json():\n",
        "  with open('char2idx.json', 'r') as f:\n",
        "    char2idx = json.load(f)\n",
        "  with open('idx2char.json', 'r') as f:\n",
        "    idx2char = json.load(f)\n",
        "  return char2idx, idx2char"
      ],
      "metadata": {
        "id": "ZlN7_rWSo5tk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def load_dicts_from_json():\n",
        "    # Load word2idx from JSON\n",
        "    with open('word2idx.json', 'r') as f:\n",
        "        word2idx = json.load(f)\n",
        "\n",
        "    # Load state2idx from JSON\n",
        "    with open('state2idx.json', 'r') as f:\n",
        "        state2idx = json.load(f)\n",
        "\n",
        "    # Load idx2word from JSON\n",
        "    with open('idx2word.json', 'r') as f:\n",
        "        idx2word = json.load(f)\n",
        "        idx2word = {int(k): v for k, v in idx2word.items()}\n",
        "\n",
        "    # Load idx2state from JSON\n",
        "    with open('idx2state.json', 'r') as f:\n",
        "        idx2state = json.load(f)\n",
        "        idx2state = {int(k): v for k, v in idx2state.items()}\n",
        "\n",
        "    return word2idx, state2idx, idx2word, idx2state"
      ],
      "metadata": {
        "id": "FJj0Q-xIGDVv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dicts_to_json(word2idx, state2idx, idx2word, idx2state)"
      ],
      "metadata": {
        "id": "Q1TpeC5pFgWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx, state2idx, idx2word, idx2state = load_dicts_from_json()"
      ],
      "metadata": {
        "id": "dWjCxP4qCmgm"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char2idx, idx2char = create_char_vocab(sentences)"
      ],
      "metadata": {
        "id": "oQbNhSiLohZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_char_dicts_to_json(char2idx, idx2char)"
      ],
      "metadata": {
        "id": "oFzyauGvpRkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import torch\n",
        "def label_weights(data, state2idx):\n",
        "    \"\"\"\n",
        "    Assigns weights to labels in a dataset based on their inverse frequency of appearance\n",
        "    in the entire label set.\n",
        "\n",
        "    Parameters:\n",
        "    data List[List[Str]]: A list of lists of labels for the dataset.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary of label weights.\n",
        "    \"\"\"\n",
        "    # Count the number of occurrences of each label in the dataset.\n",
        "    data = sum(data, [])\n",
        "    label_counts = Counter(data)\n",
        "\n",
        "    # Compute the total number of labels in the dataset.\n",
        "    total_labels = sum(label_counts.values())\n",
        "\n",
        "    # Compute the inverse frequency of appearance for each label.\n",
        "    label_weights = {label: total_labels / count for label, count in label_counts.items()}\n",
        "\n",
        "    # Normalize the weights so that they sum to 1.\n",
        "    total_weights = sum(label_weights.values())\n",
        "    label_weights = {label: weight / total_weights for label, weight in label_weights.items()}\n",
        "    \n",
        "    weight_tensor = torch.zeros(len(state2idx))\n",
        "    for label, weight in label_weights.items():\n",
        "        weight_tensor[state2idx[label]] = weight\n",
        "\n",
        "    return weight_tensor"
      ],
      "metadata": {
        "id": "z2r4EEs6UsJj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_weights = label_weights(states, state2idx)"
      ],
      "metadata": {
        "id": "lPhaGottVrtT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "pqTJX408_lSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "504wjfAf0R03",
        "outputId": "2aa8dee6-8ab5-4b28-f3a7-233acb4f7b1c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple\n",
        "\n",
        "class SentencesDataset(Dataset):\n",
        "  def __init__(self, sentences: List[str], states: List[List[str]], all_states: List[str], vocab: List[str], state2idx, word2idx, idx2word):\n",
        "    self.sentences = sentences\n",
        "    self.states = states\n",
        "    self.all_states = all_states\n",
        "    self.vocab = vocab\n",
        "    self.state2idx = state2idx\n",
        "    self.word2idx = word2idx\n",
        "    self.idx2word = idx2word\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sentences)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    sentence = self.sentences[idx]\n",
        "    encoded_sentence = [self.word2idx[word] for word in sentence]\n",
        "\n",
        "    tags = self.states[idx]\n",
        "    tags_encoded = []\n",
        "    for tag in tags:\n",
        "      one_hot = torch.zeros(len(self.all_states) + 1)\n",
        "      one_hot[self.state2idx[tag]] = 1\n",
        "      tags_encoded.append(one_hot)\n",
        "    \n",
        "    return encoded_sentence, tags_encoded\n",
        "\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "EwHPiTcJgZ_C"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharactersDataset(Dataset):\n",
        "  def __init__(self, sentences: List[str], states: List[List[str]], all_states: List[str], vocab: List[str], state2idx, char2idx, idx2char):\n",
        "    self.sentences = sentences\n",
        "    self.states = states\n",
        "    self.all_states = all_states\n",
        "    self.vocab = vocab\n",
        "    self.state2idx = state2idx\n",
        "    self.char2idx = char2idx\n",
        "    self.idx2char = idx2char\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sentences)\n",
        "  \n",
        "  def _get_chars(self, sentence):\n",
        "    chars = []\n",
        "    for word in sentence:\n",
        "      for char in word:\n",
        "        chars.append(char)\n",
        "    return chars\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    sentence = self.sentences[idx]\n",
        "    chars = self._get_chars(sentence)\n",
        "    encoded_sentence = [self.char2idx[char] for char in chars]\n",
        "\n",
        "    tags = self.states[idx]\n",
        "    tags_encoded = []\n",
        "    for tag in tags:\n",
        "      one_hot = torch.zeros(len(self.all_states) + 1)\n",
        "      one_hot[self.state2idx[tag]] = 1\n",
        "      tags_encoded.append(one_hot)\n",
        "    \n",
        "    return encoded_sentence, tags_encoded\n"
      ],
      "metadata": {
        "id": "MYt9QRxbsZ6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    # Get the sequences and labels from the batch\n",
        "    sequences, labels = zip(*batch)\n",
        "\n",
        "    # Find the maximum sequence length in the batch\n",
        "    max_len = max([len(seq) for seq in sequences])\n",
        "\n",
        "    # Pad the sequences and labels to the maximum length\n",
        "    padded_sequences = pad_sequence([torch.tensor(seq) for seq in sequences], batch_first=True, padding_value=0)\n",
        "    \n",
        "    for label in labels:\n",
        "      while len(label) < max_len:\n",
        "          zeros = torch.zeros(10)\n",
        "          zeros[0] = 1\n",
        "          label.append(zeros)\n",
        "    labels = torch.stack([torch.stack(lst) for lst in labels])\n",
        "    return padded_sequences, labels"
      ],
      "metadata": {
        "id": "FF0g6ccZHkKi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SentencesDataset(sentences, states, all_states, vocab, state2idx = state2idx, word2idx = word2idx, idx2word = idx2word)"
      ],
      "metadata": {
        "id": "9zhZgDsRicys"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_chars = CharactersDataset(sentences, states, all_states, vocab, state2idx = state2idx, idx2char = idx2char, char2idx = char2idx)"
      ],
      "metadata": {
        "id": "rMR1-4WQtbu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False, collate_fn=lambda batch: collate_fn(batch))"
      ],
      "metadata": {
        "id": "Aodoo-dnAT8t"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_chars =  DataLoader(train_dataset_chars, batch_size=2, shuffle=False, collate_fn=lambda batch: collate_fn(batch))"
      ],
      "metadata": {
        "id": "1fYZkbztvxlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_dataset = SentencesDataset(dev_sentences, dev_states, all_states, vocab, state2idx = state2idx, word2idx = word2idx, idx2word = idx2word)"
      ],
      "metadata": {
        "id": "5sW7AJwzGizZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_dataloader = DataLoader(dev_dataset, batch_size=1, shuffle=False, collate_fn=lambda batch: collate_fn(batch))"
      ],
      "metadata": {
        "id": "rvv_dHvxGocE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SentencesDataset(test_sentences, test_states, all_states, vocab, state2idx = state2idx, word2idx = word2idx, idx2word = idx2word)"
      ],
      "metadata": {
        "id": "wUuRd4ibxvwO"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=lambda batch: collate_fn(batch))"
      ],
      "metadata": {
        "id": "0aTOqdLfx8lP"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer Class and Inference Method"
      ],
      "metadata": {
        "id": "BFVMjBUT_ukP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import gc\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, training_data, device, lr=1e-3, epochs=200, betas=(0.9, 0.999), warmup_epochs=1, optim_type = 'SGD', validation_file = None, validation_data = None, label_weights = None):\n",
        "        self.hyperparams = {\n",
        "            'lr': lr,\n",
        "            'epochs': epochs,\n",
        "            'betas': betas,\n",
        "            'warmup_epochs': warmup_epochs\n",
        "        }\n",
        "        self.td = training_data\n",
        "        self.device = device\n",
        "        self.label_weights = label_weights.to(device)\n",
        "        self.loss_fn = nn.CrossEntropyLoss(weight=self.label_weights, ignore_index=0)\n",
        "        self.model = model\n",
        "        self.optim_type = optim_type\n",
        "        \n",
        "        if self.optim_type == 'Adam':\n",
        "          self.optim = torch.optim.Adam(self.model.parameters(), lr=lr, betas=self.hyperparams['betas'])\n",
        "        \n",
        "        elif self.optim_type == 'SGD':\n",
        "          self.optim = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
        "        \n",
        "        \n",
        "          \n",
        "        self.validation_file = validation_file\n",
        "        self.validation_data = validation_data\n",
        "\n",
        "    def load_optim_state_dict(self, checkpoint):\n",
        "      self.optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    def get_lr(self, epoch):\n",
        "        if epoch < self.hyperparams['warmup_epochs']:\n",
        "            return self.hyperparams['lr'] * epoch / self.hyperparams['warmup_epochs']\n",
        "        else:\n",
        "            return self.hyperparams['lr']\n",
        "  \n",
        "\n",
        "    def calc_accuracy(self, output, y):\n",
        "        pred = torch.argmax(output, dim=1)\n",
        "        y = torch.argmax(y, dim=1)\n",
        "        return (pred == y).sum().item() / len(y)\n",
        "\n",
        "    def calc_accuracy_test(self, output, y):\n",
        "        pred = torch.argmax(output, dim=1)\n",
        "        return (pred == y).sum().item() / len(y)\n",
        "\n",
        "    def train_epoch(self):\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "\n",
        "        for sentence, label in self.td:\n",
        "            self.optim.zero_grad()\n",
        "            sentence = sentence.long().to(self.device)\n",
        "            label = label.to(self.device)\n",
        "            output = self.model(sentence)\n",
        "\n",
        "            # reshape output and label to (batch_size*seq_len, # of classes)\n",
        "            output = output.view(-1, output.size(-1))\n",
        "            label = label.view(-1, label.size(-1))\n",
        "\n",
        "            # compute cross-entropy loss\n",
        "            loss = self.loss_fn(output, torch.argmax(label, dim=1))\n",
        "\n",
        "            loss.backward()\n",
        "            self.optim.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_acc += self.calc_accuracy(output, label)\n",
        "\n",
        "            del sentence, label, output\n",
        "\n",
        "        train_loss = running_loss / len(self.td)\n",
        "        training_acc = running_acc / len(self.td)\n",
        "        return train_loss, training_acc\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def validate_epoch_with_script(self):\n",
        "        if self.validation_file == None or self.validation_data == None:\n",
        "          pass\n",
        "        else:\n",
        "          all_sentences = []\n",
        "          test_output = []\n",
        "          true_res = []\n",
        "          for x, y in self.validation_data:\n",
        "            x = x.long().to(device)\n",
        "            y = y.to(device).float()\n",
        "            all_sentences.append(x)\n",
        "            output = self.model(x)\n",
        "            output = output.view(-1, output.size(-1))\n",
        "            y = y.view(-1, y.size(-1))\n",
        "            pred = torch.argmax(output, dim=1)\n",
        "            y = torch.argmax(y, dim=1)\n",
        "            test_output.append(pred)\n",
        "            true_res.append(y)\n",
        "            del x, y, output\n",
        "          decoded_sentences, decoded_tags_out, decoded_tags_true = detach(all_sentences, test_output, true_res)\n",
        "          decoded_sentences_blstm2, decoded_tags_out_blstm2, decoded_tags_true_blstm2 = decode_sentences_and_tokens(decoded_sentences, decoded_tags_out, decoded_tags_true)\n",
        "          write_inference_to_file(decoded_tags_out_blstm2, decoded_tags_true_blstm2, decoded_sentences_blstm2, \"blstm2_dev1.out\")\n",
        "          batcmd = f\"perl {self.validation_file} < blstm2_dev1.out\"\n",
        "          result = subprocess.check_output(batcmd, shell=True)\n",
        "          print(result)\n",
        "\n",
        "\n",
        "    def fit(self):\n",
        "        train_losses,train_accs = [], []\n",
        "        min_vl = float(\"-inf\")\n",
        "\n",
        "        for epoch in range(self.hyperparams['epochs']):\n",
        "            print(f\"------EPOCH {epoch+1}/{self.hyperparams['epochs']}------\")\n",
        "\n",
        "            self.model.train()\n",
        "\n",
        "            lr = self.get_lr(epoch)\n",
        "\n",
        "\n",
        "            train_loss, train_acc = self.train_epoch()\n",
        "            train_losses.append(train_loss)\n",
        "            train_accs.append(train_acc)\n",
        "\n",
        "            print(f\"Training LOSS: {train_loss} | ACCURACY: {train_acc} | LR: {lr}\")\n",
        "\n",
        "            if epoch % 3 == 0:\n",
        "              self.validate_epoch_with_script()\n",
        "              torch.save({'epoch': epoch + 1,\n",
        "                        'model_state_dict': self.model.state_dict(),\n",
        "                        'optimizer_state_dict': self.optim.state_dict(),\n",
        "                        'loss': train_loss}, \n",
        "                        'cmps544_hw4_bstml_model_simple_checkpoint.pth')\n",
        "              torch.save(self.model, \"b_lstm_model_simple.pt\")\n",
        "            cmd = \"cp cmps544_hw4_bstml_model_simple_checkpoint.pth /content/drive/MyDrive/Colab/\"\n",
        "            cmd2 = \"cp b_lstm_model_simple.pt /content/drive/MyDrive/Colab/\"\n",
        "            result = subprocess.check_output(cmd, shell=True)\n",
        "            result = subprocess.check_output(cmd2, shell=True)\n",
        "\n",
        "\n",
        "            # CLEANUP\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return (train_losses, train_accs)\n"
      ],
      "metadata": {
        "id": "5THEvNZO_x2n"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer(model, test_dataloader, trainer = None):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    test_output = []\n",
        "    true_res = []\n",
        "    incorrect_examples = []\n",
        "    incorrect_labels = []\n",
        "    incorrect_pred = []\n",
        "    all_x = []\n",
        "\n",
        "    for x, y in test_dataloader:\n",
        "\n",
        "        x = x.long().to(device)\n",
        "        y = y.to(device).float()\n",
        "        all_x.append(x)\n",
        "        output = model(x)\n",
        "        output = output.view(-1, output.size(-1))\n",
        "        y = y.view(-1, y.size(-1))\n",
        "        pred = torch.argmax(output, dim=1)\n",
        "        y = torch.argmax(y, dim=1)\n",
        "        test_output.append(pred)\n",
        "        true_res.append(y)\n",
        "        if trainer:\n",
        "          running_acc += trainer.calc_accuracy_test(output, y)\n",
        "          loss = trainer.loss_fn(output, y)\n",
        "          running_loss += loss.item()\n",
        "        del x, y, output\n",
        "\n",
        "    test_loss = running_loss / len(test_dataloader)\n",
        "    test_acc = running_acc / len(test_dataloader)\n",
        "\n",
        "    # calculate F1 score, precision, and recall\n",
        "    test_output_cf = torch.cat(test_output, dim=0).cpu().numpy()\n",
        "    true_res_cf = torch.cat(true_res, dim=0).cpu().numpy()\n",
        "\n",
        "    conf_mat = confusion_matrix(true_res_cf, test_output_cf)\n",
        "    true_positives = np.diag(conf_mat)\n",
        "    false_positives = np.sum(conf_mat, axis=0) - true_positives\n",
        "    false_negatives = np.sum(conf_mat, axis=1) - true_positives\n",
        "\n",
        "    precision = np.sum(true_positives) / (np.sum(true_positives) + np.sum(false_positives))\n",
        "    recall = np.sum(true_positives) / (np.sum(true_positives) + np.sum(false_negatives))\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "    return test_loss, test_acc, test_output, true_res, f1, precision, recall, all_x\n"
      ],
      "metadata": {
        "id": "Vkv2xVP2ABcN"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "35FhsW2B67FO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tXzq-sm98UmJ",
        "outputId": "555a7391-4005-4d9b-86d2-a0ac09b35e3c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Simple Bidirectional LSTM"
      ],
      "metadata": {
        "id": "XV0_HDhV8wjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleBidirectionalLSTM(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, hidden_dim, n_layers, device, dropout_pct, num_classes):\n",
        "        super(SimpleBidirectionalLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.dropout_pct = dropout_pct\n",
        "\n",
        "        self.bilstm = nn.LSTM(input_size=embedding_dim,\n",
        "                              hidden_size=hidden_dim,\n",
        "                              num_layers=n_layers,\n",
        "                              batch_first=True,\n",
        "                              bidirectional=True)\n",
        "\n",
        "        # Linear layer\n",
        "        self.linear = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "        # ELU activation function\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(self.dropout_pct)\n",
        "\n",
        "        # Classifier layer\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # print(text)\n",
        "        # print(text.shape)\n",
        "        # Embedding layer\n",
        "        embedded = self.embedding_layer(text)\n",
        "\n",
        "        # BiLSTM layer\n",
        "        bilstm_output, _ = self.bilstm(embedded)\n",
        "\n",
        "        # Apply dropout layer\n",
        "        dropped = self.dropout(bilstm_output)\n",
        "\n",
        "        # Apply linear layer\n",
        "        linear_output = self.linear(dropped)\n",
        "\n",
        "        # Apply ELU activation function\n",
        "        elu_output = self.elu(linear_output)\n",
        "\n",
        "        # Apply classifier layer to every time step\n",
        "        output = self.classifier(elu_output)\n",
        "        #print(output.shape)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "nxMXRYpM5yyX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b_lstm_model = SimpleBidirectionalLSTM(len(word2idx.keys()), 100, 256, 1, device, 0.33, len(state2idx.keys())).to(device)\n",
        "print(b_lstm_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeP8rsHY7drY",
        "outputId": "c69273cd-b01f-4b57-af36-df3c069a72be"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleBidirectionalLSTM(\n",
            "  (embedding_layer): Embedding(30291, 100)\n",
            "  (bilstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (elu): ELU(alpha=1.0)\n",
            "  (dropout): Dropout(p=0.33, inplace=False)\n",
            "  (classifier): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b_lstm_model.load_state_dict(torch.load(\"/content/cmps544_hw4_bstml_model_simple_checkpoint.pth\")[\"model_state_dict\"])\n",
        "b_lstm_model.to(device)"
      ],
      "metadata": {
        "id": "oBrYlvXgKsUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ba87252-12c7-4b3f-d0e6-6915f55c7d32"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleBidirectionalLSTM(\n",
              "  (embedding_layer): Embedding(30291, 100)\n",
              "  (bilstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
              "  (linear): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (elu): ELU(alpha=1.0)\n",
              "  (dropout): Dropout(p=0.33, inplace=False)\n",
              "  (classifier): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_blstm = Trainer(b_lstm_model, train_dataloader, device, epochs=200, warmup_epochs=1, validation_data = dev_dataloader, validation_file = \"conll03eval\", lr = 1e-1, label_weights=label_weights)"
      ],
      "metadata": {
        "id": "wEpAIkUE_d2U"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_losses, train_accs) = trainer_blstm.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyQaerOnAn20",
        "outputId": "ffe14dd8-05ea-4ec3-9b3c-78c56f9c2556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------EPOCH 1/200------\n",
            "Training LOSS: 0.00291117375813826 | ACCURACY: 0.40638496740024765 | LR: 0.0\n",
            "b'processed 51578 tokens with 5942 phrases; found: 5927 phrases; correct: 4271.\\naccuracy:  94.26%; precision:  72.06%; recall:  71.88%; FB1:  71.97\\n              LOC: precision:  85.79%; recall:  78.55%; FB1:  82.01  1682\\n             MISC: precision:  72.76%; recall:  74.73%; FB1:  73.73  947\\n              ORG: precision:  62.40%; recall:  68.68%; FB1:  65.39  1476\\n              PER: precision:  66.85%; recall:  66.12%; FB1:  66.48  1822\\n'\n",
            "------EPOCH 2/200------\n",
            "Training LOSS: 0.0026215796066134988 | ACCURACY: 0.40649013539541673 | LR: 0.1\n",
            "------EPOCH 3/200------\n",
            "Training LOSS: 0.002785542371687754 | ACCURACY: 0.40641794898813927 | LR: 0.1\n",
            "------EPOCH 4/200------\n",
            "Training LOSS: 0.002573491708530303 | ACCURACY: 0.4065614227579458 | LR: 0.1\n",
            "b'processed 51578 tokens with 5942 phrases; found: 5948 phrases; correct: 4306.\\naccuracy:  94.30%; precision:  72.39%; recall:  72.47%; FB1:  72.43\\n              LOC: precision:  84.89%; recall:  78.93%; FB1:  81.81  1708\\n             MISC: precision:  73.78%; recall:  75.70%; FB1:  74.73  946\\n              ORG: precision:  63.52%; recall:  68.83%; FB1:  66.07  1453\\n              PER: precision:  67.08%; recall:  67.05%; FB1:  67.06  1841\\n'\n",
            "------EPOCH 5/200------\n",
            "Training LOSS: 0.002156416575721642 | ACCURACY: 0.4065152718567924 | LR: 0.1\n",
            "------EPOCH 6/200------\n",
            "Training LOSS: 0.0022978960236343937 | ACCURACY: 0.4065677638577596 | LR: 0.1\n",
            "------EPOCH 7/200------\n",
            "Training LOSS: 0.0022371883487593315 | ACCURACY: 0.4066388206539078 | LR: 0.1\n",
            "b'processed 51578 tokens with 5942 phrases; found: 5908 phrases; correct: 4313.\\naccuracy:  94.35%; precision:  73.00%; recall:  72.58%; FB1:  72.79\\n              LOC: precision:  83.80%; recall:  79.70%; FB1:  81.70  1747\\n             MISC: precision:  74.76%; recall:  75.49%; FB1:  75.12  931\\n              ORG: precision:  65.16%; recall:  68.90%; FB1:  66.98  1418\\n              PER: precision:  67.83%; recall:  66.72%; FB1:  67.27  1812\\n'\n",
            "------EPOCH 8/200------\n",
            "Training LOSS: 0.002476571343019951 | ACCURACY: 0.40652132137451624 | LR: 0.1\n",
            "------EPOCH 9/200------\n",
            "Training LOSS: 0.0021557868390158223 | ACCURACY: 0.40660253193708734 | LR: 0.1\n",
            "------EPOCH 10/200------\n",
            "Training LOSS: 0.002368927267076527 | ACCURACY: 0.4065775308179706 | LR: 0.1\n",
            "b'processed 51578 tokens with 5942 phrases; found: 5912 phrases; correct: 4336.\\naccuracy:  94.42%; precision:  73.34%; recall:  72.97%; FB1:  73.16\\n              LOC: precision:  86.65%; recall:  79.86%; FB1:  83.12  1693\\n             MISC: precision:  72.27%; recall:  74.62%; FB1:  73.43  952\\n              ORG: precision:  63.47%; recall:  68.53%; FB1:  65.90  1448\\n              PER: precision:  69.38%; recall:  68.51%; FB1:  68.94  1819\\n'\n",
            "------EPOCH 11/200------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(b_lstm_model.state_dict(), 'blstm1.pth')"
      ],
      "metadata": {
        "id": "mL2LBV_ndaTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b_lstm_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEGYi9QgxLOL",
        "outputId": "23118f03-ccda-42e5-9113-280cfd1265ca"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleBidirectionalLSTM(\n",
              "  (embedding_layer): Embedding(30291, 100)\n",
              "  (bilstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
              "  (linear): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (elu): ELU(alpha=1.0)\n",
              "  (dropout): Dropout(p=0.33, inplace=False)\n",
              "  (classifier): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(b_lstm_model, \"blstm1.pt\")"
      ],
      "metadata": {
        "id": "o2edj2Nct5wC"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_loss, dev_acc, dev_output, dev_true_res, dev_f1, dev_precision, dev_recall, all_sentences = infer(model=b_lstm_model, test_dataloader=dev_dataloader)"
      ],
      "metadata": {
        "id": "OBbQe6kZF6Ln"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test1_loss, test1_acc, test1_output, test1_true_res, test1_f1, test1_precision, test1_recall, test_sentences = infer(model=b_lstm_model, test_dataloader=test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPgve5ivyV7y",
        "outputId": "5fb71e70-d362-4ae8-f5fd-ef536f058e6f"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-123-6a998615bb86>:48: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  f1 = 2 * precision * recall / (precision + recall)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'F1 Score on Dev set: {dev_f1}')\n",
        "print(f'Precision on Dev set: {dev_precision}')\n",
        "print(f'Recall on Dev set: {dev_recall}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEoUqtlYUkBz",
        "outputId": "e23edc43-cf69-484e-9b1b-a51c37199110"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score on Dev set: 0.9518011555314281\n",
            "Precision on Dev set: 0.9518011555314281\n",
            "Recall on Dev set: 0.9518011555314281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentences, decoded_tags_out, decoded_tags_true = detach(all_sentences, dev_output, dev_true_res)"
      ],
      "metadata": {
        "id": "Df3QMa56o5xy"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentences, decoded_tags_out, decoded_tags_true = decode_sentences_and_tokens(decoded_sentences, decoded_tags_out, decoded_tags_true)"
      ],
      "metadata": {
        "id": "XaYCGMzfcqKV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentences_t, decoded_tags_out_t, decoded_tags_true_t = detach(test_sentences, test1_output, test1_true_res)"
      ],
      "metadata": {
        "id": "CPQtsHBx0rkx"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentences_t, decoded_tags_out_t, decoded_tags_true_t = decode_sentences_and_tokens(decoded_sentences_t, decoded_tags_out_t, decoded_tags_true_t)"
      ],
      "metadata": {
        "id": "-ykkyKTc1quX"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_inference_to_file(decoded_tags_out, decoded_tags_true, decoded_sentences, \"dev1.out\", eval = False)"
      ],
      "metadata": {
        "id": "VubBraCQjjc9"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_inference_to_file(decoded_tags_out_t, decoded_tags_true_t, decoded_sentences_t, \"test1.out\", eval = False)"
      ],
      "metadata": {
        "id": "VHlgRl2Q14h1"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!perl conll03eval < dev1.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRvUMfS8lw1v",
        "outputId": "83b97a43-0b06-4fae-9d66-d52b859298f2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51578 tokens with 5942 phrases; found: 5567 phrases; correct: 4425.\n",
            "accuracy:  95.18%; precision:  79.49%; recall:  74.47%; FB1:  76.90\n",
            "              LOC: precision:  89.78%; recall:  81.33%; FB1:  85.35  1664\n",
            "             MISC: precision:  83.20%; recall:  76.79%; FB1:  79.86  851\n",
            "              ORG: precision:  71.55%; recall:  70.69%; FB1:  71.12  1325\n",
            "              PER: precision:  73.83%; recall:  69.22%; FB1:  71.45  1727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: BLSTM with GloVE Embeddings"
      ],
      "metadata": {
        "id": "1ofmdUZ2pH07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 101"
      ],
      "metadata": {
        "id": "Nn_0CA53x0SN"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "def load_glove_embeddings(embedding_file_path):\n",
        "    embeddings_index = {}\n",
        "    with gzip.open(embedding_file_path, 'rt', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index\n",
        "\n",
        "glove_embeddings = load_glove_embeddings('/content/drive/MyDrive/Colab/glove.6B.100d.gz')\n"
      ],
      "metadata": {
        "id": "1YcsJEZPxs0j"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_matrix(word_index, embeddings_index, embedding_dim):\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))    \n",
        "    np.random.seed(123)  # Set a fixed seed for reproducibility\n",
        "    mean = 0\n",
        "    stddev = 1\n",
        "    unk_vec = np.random.normal(mean, stddev, size=(100,))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word.lower())\n",
        "        if embedding_vector is None:\n",
        "          embedding_vector = unk_vec        \n",
        "        casing_encoding = np.array([int(word[0].isupper())])\n",
        "        combined_vector = np.hstack((embedding_vector, casing_encoding))\n",
        "        embedding_matrix[i] = combined_vector\n",
        "    return embedding_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "iVAINuo0GKzd"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_mat = create_embedding_matrix(word2idx, glove_embeddings, EMBEDDING_DIM)"
      ],
      "metadata": {
        "id": "3bUo8EAJ1Ove"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GloVEBidirectionalLSTM(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, hidden_dim, n_layers, device, dropout_pct, num_classes, embedding_mat):\n",
        "        super(GloVEBidirectionalLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.device = device\n",
        "      \n",
        "\n",
        "        # Create embedding layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat), freeze=True)\n",
        "        self.dropout_pct = dropout_pct\n",
        "\n",
        "        self.bilstm = nn.LSTM(input_size=embedding_dim,\n",
        "                              hidden_size=hidden_dim,\n",
        "                              num_layers=n_layers,\n",
        "                              batch_first=True,\n",
        "                              bidirectional=True)\n",
        "\n",
        "        # Linear layer\n",
        "        self.linear = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "        # ELU activation function\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(self.dropout_pct)\n",
        "\n",
        "        # Classifier layer\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # print(text)\n",
        "        # print(text.shape)\n",
        "        # Embedding layer\n",
        "        embedded = self.embedding_layer(text)\n",
        "\n",
        "        # BiLSTM layer\n",
        "        bilstm_output, _ = self.bilstm(embedded)\n",
        "\n",
        "        # Apply dropout layer\n",
        "        dropped = self.dropout(bilstm_output)\n",
        "\n",
        "        # Apply linear layer\n",
        "        linear_output = self.linear(dropped)\n",
        "\n",
        "        # Apply ELU activation function\n",
        "        elu_output = self.elu(linear_output)\n",
        "\n",
        "        # Apply classifier layer to every time step\n",
        "        output = self.classifier(elu_output)\n",
        "        #print(output.shape)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "tFwearIOpPEY"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b_lstm_model_glove = GloVEBidirectionalLSTM(len(word2idx.keys()), EMBEDDING_DIM, 256, 1, device, 0.33, len(state2idx.keys()), embedding_mat).to(device)"
      ],
      "metadata": {
        "id": "pV0DfmCX1sTh"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b_lstm_model_glove.load_state_dict(torch.load(\"/content/cmps544_hw4_bstml_model_checkpoint.pth\")[\"model_state_dict\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1UygeV84mV2",
        "outputId": "0e5fc892-2833-438c-fc9b-30f9b5dc9392"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_blstm_2 = Trainer(b_lstm_model_glove, train_dataloader, device, epochs=100, warmup_epochs=20, validation_data = dev_dataloader, validation_file = \"conll03eval\", lr = 1e-3, optim_type = 'SGD', label_weights=label_weights)"
      ],
      "metadata": {
        "id": "ue8qtTh828sI"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_losses, train_accs) = trainer_blstm_2.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "33uXhmTV3Iev",
        "outputId": "25503423-7e07-4b98-c001-0ebe07f86863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------EPOCH 1/100------\n",
            "Training LOSS: 0.027388364530295473 | ACCURACY: 0.9917700161406517 | LR: 0.0\n",
            "b'processed 51578 tokens with 5942 phrases; found: 6101 phrases; correct: 5248.\\naccuracy:  97.91%; precision:  86.02%; recall:  88.32%; FB1:  87.15\\n              LOC: precision:  89.09%; recall:  93.30%; FB1:  91.15  1924\\n             MISC: precision:  76.70%; recall:  80.69%; FB1:  78.65  970\\n              ORG: precision:  79.67%; recall:  78.90%; FB1:  79.28  1328\\n              PER: precision:  92.18%; recall:  94.03%; FB1:  93.09  1879\\n'\n",
            "------EPOCH 2/100------\n",
            "Training LOSS: 0.027832139971334085 | ACCURACY: 0.9914834325154747 | LR: 5e-05\n",
            "------EPOCH 3/100------\n",
            "Training LOSS: 0.02816464930293323 | ACCURACY: 0.991720224221421 | LR: 0.0001\n",
            "------EPOCH 4/100------\n",
            "Training LOSS: 0.02814234041544249 | ACCURACY: 0.9912678122019191 | LR: 0.00015000000000000001\n",
            "b'processed 51578 tokens with 5942 phrases; found: 6141 phrases; correct: 5276.\\naccuracy:  97.98%; precision:  85.91%; recall:  88.79%; FB1:  87.33\\n              LOC: precision:  89.98%; recall:  93.41%; FB1:  91.67  1907\\n             MISC: precision:  76.20%; recall:  81.24%; FB1:  78.64  983\\n              ORG: precision:  79.48%; recall:  79.42%; FB1:  79.45  1340\\n              PER: precision:  91.37%; recall:  94.79%; FB1:  93.05  1911\\n'\n",
            "------EPOCH 5/100------\n",
            "Training LOSS: 0.02746922260051105 | ACCURACY: 0.9916541687305513 | LR: 0.0002\n",
            "------EPOCH 6/100------\n",
            "Training LOSS: 0.028185845332748698 | ACCURACY: 0.9911649686298175 | LR: 0.00025\n",
            "------EPOCH 7/100------\n",
            "Training LOSS: 0.02707261808200963 | ACCURACY: 0.991861357190566 | LR: 0.00030000000000000003\n",
            "b'processed 51578 tokens with 5942 phrases; found: 6137 phrases; correct: 5280.\\naccuracy:  98.00%; precision:  86.04%; recall:  88.86%; FB1:  87.42\\n              LOC: precision:  89.45%; recall:  93.25%; FB1:  91.31  1915\\n             MISC: precision:  76.26%; recall:  82.21%; FB1:  79.12  994\\n              ORG: precision:  79.04%; recall:  79.87%; FB1:  79.45  1355\\n              PER: precision:  92.79%; recall:  94.35%; FB1:  93.57  1873\\n'\n",
            "------EPOCH 8/100------\n",
            "Training LOSS: 0.02733133673433224 | ACCURACY: 0.9915370899207436 | LR: 0.00035\n",
            "------EPOCH 9/100------\n",
            "Training LOSS: 0.026648480832327903 | ACCURACY: 0.9920787964890753 | LR: 0.0004\n",
            "------EPOCH 10/100------\n",
            "Training LOSS: 0.027043480788674582 | ACCURACY: 0.9915629212843274 | LR: 0.00045000000000000004\n",
            "b'processed 51578 tokens with 5942 phrases; found: 6105 phrases; correct: 5238.\\naccuracy:  97.91%; precision:  85.80%; recall:  88.15%; FB1:  86.96\\n              LOC: precision:  89.98%; recall:  92.92%; FB1:  91.43  1897\\n             MISC: precision:  76.52%; recall:  79.18%; FB1:  77.83  954\\n              ORG: precision:  78.87%; recall:  79.34%; FB1:  79.11  1349\\n              PER: precision:  91.18%; recall:  94.30%; FB1:  92.71  1905\\n'\n",
            "------EPOCH 11/100------\n",
            "Training LOSS: 0.026715394027067685 | ACCURACY: 0.9920079653 | LR: 0.0005\n",
            "------EPOCH 12/100------\n",
            "Training LOSS: 0.026433380403636204 | ACCURACY: 0.9919800975160026 | LR: 0.0005499999999999999\n",
            "------EPOCH 13/100------\n",
            "Training LOSS: 0.026489466108172773 | ACCURACY: 0.9920288673398506 | LR: 0.0006000000000000001\n",
            "b'processed 51578 tokens with 5942 phrases; found: 6072 phrases; correct: 5247.\\naccuracy:  97.90%; precision:  86.41%; recall:  88.30%; FB1:  87.35\\n              LOC: precision:  91.25%; recall:  92.54%; FB1:  91.89  1863\\n             MISC: precision:  75.31%; recall:  80.04%; FB1:  77.60  980\\n              ORG: precision:  79.78%; recall:  80.01%; FB1:  79.90  1345\\n              PER: precision:  92.14%; recall:  94.25%; FB1:  93.18  1884\\n'\n",
            "------EPOCH 14/100------\n",
            "Training LOSS: 0.02603779003714356 | ACCURACY: 0.992070067578238 | LR: 0.0006500000000000001\n",
            "------EPOCH 15/100------\n",
            "Training LOSS: 0.026369318526762175 | ACCURACY: 0.992149525969061 | LR: 0.0007\n",
            "------EPOCH 16/100------\n",
            "Training LOSS: 0.02616581679398302 | ACCURACY: 0.9920324790173277 | LR: 0.00075\n",
            "b'processed 51578 tokens with 5942 phrases; found: 6095 phrases; correct: 5252.\\naccuracy:  97.94%; precision:  86.17%; recall:  88.39%; FB1:  87.26\\n              LOC: precision:  90.25%; recall:  92.71%; FB1:  91.46  1887\\n             MISC: precision:  74.97%; recall:  81.24%; FB1:  77.98  999\\n              ORG: precision:  80.73%; recall:  79.64%; FB1:  80.18  1323\\n              PER: precision:  91.83%; recall:  94.03%; FB1:  92.92  1886\\n'\n",
            "------EPOCH 17/100------\n",
            "Training LOSS: 0.026078135874550823 | ACCURACY: 0.9921476132935283 | LR: 0.0008\n",
            "------EPOCH 18/100------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-99b787b0bb3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_blstm_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-65-970d50553e97>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mtrain_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-970d50553e97>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b_lstm_model_glove.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Rd4n5_s57Sn",
        "outputId": "b4588e05-e9bf-4a48-ba92-c9a663208c9e"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GloVEBidirectionalLSTM(\n",
              "  (embedding_layer): Embedding(30292, 101)\n",
              "  (bilstm): LSTM(101, 256, batch_first=True, bidirectional=True)\n",
              "  (linear): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (elu): ELU(alpha=1.0)\n",
              "  (dropout): Dropout(p=0.33, inplace=False)\n",
              "  (classifier): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(b_lstm_model_glove, \"blstm2.pt\")"
      ],
      "metadata": {
        "id": "QEFIe36LWhX5"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_loss_blstm2, dev_acc_blstm2, dev_output_blstm2, dev_true_res_blstm2, dev_f1_blstm2, dev_precision_blstm2, dev_recall_blstm2, all_sentences_blstm2 = infer(trainer_blstm_2, b_lstm_model_glove, dev_dataloader)"
      ],
      "metadata": {
        "id": "9KOceZBiXEDe"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentences_blstm2, decoded_tags_out_blstm2, decoded_tags_true_blstm2 = detach(all_sentences_blstm2, dev_output_blstm2, dev_true_res_blstm2)\n",
        "decoded_sentences_blstm2, decoded_tags_out_blstm2, decoded_tags_true_blstm2 = decode_sentences_and_tokens(decoded_sentences_blstm2, decoded_tags_out_blstm2, decoded_tags_true_blstm2)"
      ],
      "metadata": {
        "id": "bbxGqtu1GHpU"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_blstm2, test_acc_blstm2, test_output_blstm2, test_true_res_blstm2, test_f1_blstm2, test_precision_blstm2, test_recall_blstm2, test_sentences_blstm2 = infer(trainer_blstm_2, b_lstm_model_glove, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIFTzXqh6plF",
        "outputId": "e05a9321-fb66-4982-8f52-d05a3e312960"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-3abc71d0854a>:50: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  f1 = 2 * precision * recall / (precision + recall)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentences_blstm2_t, decoded_tags_out_blstm2_t, decoded_tags_true_blstm2_t = detach(test_sentences_blstm2, test_output_blstm2, test_true_res_blstm2)\n",
        "decoded_sentences_blstm2_t, decoded_tags_out_blstm2_t, decoded_tags_true_blstm2_t = decode_sentences_and_tokens(decoded_sentences_blstm2_t, decoded_tags_out_blstm2_t, decoded_tags_true_blstm2_t)"
      ],
      "metadata": {
        "id": "9cUGv0M664w4"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_inference_to_file(decoded_tags_out_blstm2, decoded_tags_true_blstm2, decoded_sentences_blstm2, \"dev2.out\", eval = False)"
      ],
      "metadata": {
        "id": "4cXtTbfDHOLF"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_inference_to_file(decoded_tags_out_blstm2_t, decoded_tags_true_blstm2_t, decoded_sentences_blstm2_t, \"test2.out\", eval = False)"
      ],
      "metadata": {
        "id": "sTfkZCf27NYD"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!perl conll03eval < blstm2_dev1.out"
      ],
      "metadata": {
        "id": "h1Ni3vfoHZ2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37ac93de-c725-4849-cc8d-7997d65c2131"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51578 tokens with 5942 phrases; found: 6044 phrases; correct: 5349.\n",
            "accuracy:  98.21%; precision:  88.50%; recall:  90.02%; FB1:  89.25\n",
            "              LOC: precision:  92.06%; recall:  93.36%; FB1:  92.70  1863\n",
            "             MISC: precision:  79.41%; recall:  84.06%; FB1:  81.66  976\n",
            "              ORG: precision:  83.36%; recall:  82.18%; FB1:  82.76  1322\n",
            "              PER: precision:  93.31%; recall:  95.39%; FB1:  94.34  1883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: BLSTM - CNN"
      ],
      "metadata": {
        "id": "e2d2qgm3lT1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GloVEBidirectionalLSTMCNN(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, hidden_dim, n_layers, device, dropout_pct, num_classes, embedding_mat):\n",
        "        super(GloVEBidirectionalLSTMCNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.device = device\n",
        "      \n",
        "\n",
        "        # Create embedding layer\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_mat), freeze=True)\n",
        "        self.dropout_pct = dropout_pct\n",
        "\n",
        "        self.bilstm = nn.LSTM(input_size=embedding_dim,\n",
        "                              hidden_size=hidden_dim,\n",
        "                              num_layers=n_layers,\n",
        "                              batch_first=True,\n",
        "                              bidirectional=True)\n",
        "\n",
        "        # Linear layer\n",
        "        self.linear = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "        # ELU activation function\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(self.dropout_pct)\n",
        "\n",
        "        # Classifier layer\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # print(text)\n",
        "        # print(text.shape)\n",
        "        # Embedding layer\n",
        "        embedded = self.embedding_layer(text)\n",
        "\n",
        "        # BiLSTM layer\n",
        "        bilstm_output, _ = self.bilstm(embedded)\n",
        "\n",
        "        # Apply dropout layer\n",
        "        dropped = self.dropout(bilstm_output)\n",
        "\n",
        "        # Apply linear layer\n",
        "        linear_output = self.linear(dropped)\n",
        "\n",
        "        # Apply ELU activation function\n",
        "        elu_output = self.elu(linear_output)\n",
        "\n",
        "        # Apply classifier layer to every time step\n",
        "        output = self.classifier(elu_output)\n",
        "        #print(output.shape)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "WW89tcS-lY4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, num_filters, filter_sizes, output_dim):\n",
        "        super(CharCNN, self).__init__()\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        \n",
        "        # Convolutional layers\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=fs)\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "        \n",
        "        # Fully-connected layer\n",
        "        self.fc = nn.Linear(len(filter_sizes) * num_filters, output_dim)\n",
        "        \n",
        "        # Activation function\n",
        "        self.activation = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Convert input to embeddings\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # Transpose tensor for Conv1d input\n",
        "        x = x.permute(0, 2, 1)\n",
        "        \n",
        "        # Apply convolutional filters and activation\n",
        "        conv_outputs = []\n",
        "        for conv in self.convs:\n",
        "            conv_output = conv(x)\n",
        "            conv_output = self.activation(conv_output)\n",
        "            conv_output = nn.functional.max_pool1d(conv_output, conv_output.shape[2])\n",
        "            conv_outputs.append(conv_output.squeeze())\n",
        "        \n",
        "        # Concatenate convolutional outputs\n",
        "        x = torch.cat(conv_outputs, dim=1)\n",
        "        \n",
        "        # Apply fully-connected layer\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "qXww3Z6cnbz1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}